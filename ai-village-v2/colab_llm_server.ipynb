{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Village - Colab LLM Server\n",
        "\n",
        "This notebook runs multiple LLMs on Colab's GPU and exposes them via Cloudflare tunnel.\n",
        "I ran this using A100 GPU. You can use a compatible GPU for your use-case. For this example, I'll use A100 for this\n",
        "\n",
        "## Instructions\n",
        "1. **Set GPU:** Runtime → Change runtime type → **A100 GPU**\n",
        "2. Run cells 1-6 in order\n",
        "3. Copy the tunnel URL from Cell 6\n",
        "4. Update `config.py` on your laptop with the new URL\n",
        "5. Run Cell 7 to keep the session alive\n",
        "\n",
        "## Models Used\n",
        "| Agent | Model | Size |\n",
        "|-------|-------|------|\n",
        "| Scout | mistral:7b | 4.1GB |\n",
        "| Conservative Engineer | llama3.1:8b | 4.7GB |\n",
        "| Quality Engineer | codellama:13b | 7.4GB |\n",
        "| Innovative Engineer | mixtral:8x7b | 26GB |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Check GPU\n",
        "# You should see an A100 or similar GPU\n",
        "\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "print(\"\\n✓ GPU is available!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Ollama\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "print(\"\\n✓ Ollama installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Start Ollama server on all interfaces\n",
        "# IMPORTANT: This binds to 0.0.0.0 so the tunnel can reach it\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Kill any existing Ollama process\n",
        "!pkill -9 -f ollama 2>/dev/null || true\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Ollama with binding to all interfaces\n",
        "env = os.environ.copy()\n",
        "env[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    env=env,\n",
        "    stdout=open('/tmp/ollama.log', 'w'),\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# Verify it's running\n",
        "!curl -s http://localhost:11434 && echo \" Ollama server ready!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Pull all models (10-15 min on first run)\n",
        "# These are cached after first download\n",
        "\n",
        "print(\"Pulling models... This takes 10-15 minutes on first run.\\n\")\n",
        "\n",
        "models = [\n",
        "    (\"mistral:7b\", \"Scout\"),\n",
        "    (\"llama3.1:8b\", \"Conservative Engineer\"),\n",
        "    (\"codellama:13b\", \"Quality Engineer\"),\n",
        "    (\"mixtral:8x7b\", \"Innovative Engineer\")\n",
        "]\n",
        "\n",
        "for model, agent in models:\n",
        "    print(f\"Pulling {model} (for {agent})...\")\n",
        "    !ollama pull {model}\n",
        "    print(f\"{model} ready\\n\")\n",
        "\n",
        "print(\"All models downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Create Cloudflare Tunnel\n",
        "# This exposes your Ollama server to the internet (no account needed)\n",
        "\n",
        "# Install cloudflared\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb > /dev/null 2>&1\n",
        "!rm cloudflared-linux-amd64.deb\n",
        "\n",
        "# Kill any existing tunnels\n",
        "!pkill -f cloudflared 2>/dev/null || true\n",
        "\n",
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Start cloudflared tunnel\n",
        "print(\"Starting Cloudflare tunnel...\")\n",
        "process = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Wait for URL\n",
        "time.sleep(3)\n",
        "tunnel_url = None\n",
        "\n",
        "for _ in range(30):\n",
        "    line = process.stdout.readline()\n",
        "    if line:\n",
        "        match = re.search(r'(https://[a-z0-9-]+\\.trycloudflare\\.com)', line)\n",
        "        if match:\n",
        "            tunnel_url = match.group(1)\n",
        "            break\n",
        "    time.sleep(1)\n",
        "\n",
        "if tunnel_url:\n",
        "    print(\"TUNNEL READY!\")\n",
        "    print(f\"\\nURL: {tunnel_url}\")\n",
        "    print(f\"\\nCopy this to your config.py:\\n\")\n",
        "    print(f'REMOTE_OLLAMA_URL = \"{tunnel_url}/api/generate\"')\n",
        "else:\n",
        "    print(\"Could not get tunnel URL. Re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Keep session alive\n",
        "# Run this and KEEP THE TAB OPEN while using the AI Village\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LLM Server is running!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nKeep this tab open. Close it to shut down the server.\")\n",
        "print(\"The dots below show the server is still active.\\n\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "try:\n",
        "    while True:\n",
        "        elapsed = datetime.now() - start_time\n",
        "        mins = int(elapsed.total_seconds() // 60)\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        if mins > 0 and mins % 10 == 0:\n",
        "            print(f\" [{mins}m]\", end=\"\", flush=True)\n",
        "        time.sleep(60)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nServer stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
